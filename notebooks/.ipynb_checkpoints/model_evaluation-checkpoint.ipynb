{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32f6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3a877d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_test, y_test, scaler\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load models and test data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m models \u001b[38;5;241m=\u001b[39m load_models()\n\u001b[1;32m     21\u001b[0m X_test, y_test, scaler \u001b[38;5;241m=\u001b[39m load_test_data()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mload_models\u001b[0;34m(models_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtitle()\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_path, filename), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 8\u001b[0m             models[model_name] \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "def load_models(models_path='../models/'):\n",
    "    \"\"\"Load all saved model pickle files\"\"\"\n",
    "    models = {}\n",
    "    for filename in os.listdir(models_path):\n",
    "        if filename.endswith('.pkl') and filename not in ['scaler.pkl', 'test_data.pkl']:\n",
    "            model_name = filename.replace('.pkl', '').replace('_', ' ').title()\n",
    "            with open(os.path.join(models_path, filename), 'rb') as f:\n",
    "                models[model_name] = pickle.load(f)\n",
    "    return models\n",
    "\n",
    "def load_test_data(models_path='../models/'):\n",
    "    \"\"\"Load test data and scaler\"\"\"\n",
    "    with open(os.path.join(models_path, 'test_data.pkl'), 'rb') as f:\n",
    "        X_test, y_test = pickle.load(f)\n",
    "    with open(os.path.join(models_path, 'scaler.pkl'), 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    return X_test, y_test, scaler\n",
    "\n",
    "# Load models and test data\n",
    "models = load_models()\n",
    "X_test, y_test, scaler = load_test_data()\n",
    "\n",
    "print(f\"âœ… Loaded {len(models)} models\")\n",
    "print(f\"âœ… Test set size: {X_test.shape}\")\n",
    "print(f\"âœ… Models: {list(models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aeb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    results[name] = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "print(\"\\nâœ… All models evaluated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26696b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1-Score': result['f1'],\n",
    "        'ROC-AUC': result['roc_auc']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "# Display with styling\n",
    "comparison_df.style.background_gradient(cmap='YlGn', subset=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'ROC-AUC']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    data = comparison_df.sort_values(metric, ascending=True)\n",
    "    ax.barh(data['Model'], data[metric], color=colors[idx])\n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(data[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    if idx < len(axes):\n",
    "        sns.heatmap(result['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "        axes[idx].set_title(f'Confusion Matrix - {name}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Actual Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da417d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['fpr'], result['tpr'], label=f\"{name} (AUC = {result['roc_auc']:.3f})\", linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc31c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, result in results.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Classification Report - {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(classification_report(y_test, result['y_pred'], target_names=['Legitimate', 'Fraud']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411394c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_auc = comparison_df.iloc[0]['ROC-AUC']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"ROC-AUC Score: {best_auc:.4f}\")\n",
    "print(f\"\\nThis model shows the best overall performance for fraud detection.\")\n",
    "print(f\"Consider this model for deployment or further tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14266bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
